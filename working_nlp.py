# -*- coding: utf-8 -*-
"""working_nlp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15Ti-93d9bdHtDOiiyNBtMuaHd_YLfIpN
"""

# importing all the required libraries
import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt

# reading the dataset file and printing first 10 records
dataset = pd.read_csv('/content/housing.csv')

dataset = dataset.dropna()
print("Here are the first ten rows of the dataset: ")
dataset.head(10)

# setting the label and training features using X and Y
Y = dataset['median_house_value']

X = dataset.loc[:,'longitude':'median_income']

# splitting the train and test data to 70 and 30
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)

x_train_np = x_train.to_numpy()
y_train_np = y_train.to_numpy()

x_test_np = x_test.to_numpy()
y_test_np = y_test.to_numpy()

# plotting the subplots for the columns
# get columns to plot
columns = dataset.columns.drop(['ocean_proximity'])
# create x data
xAxis_data = range(0, 18)
# create figure and axis
fig, point = plt.subplots(9, 1)
i=0
# plot each column
for column in columns:
    point[i].plot(xAxis_data, dataset.head(18)[column], label=column)
    point[i].legend()
    i=i+1

# importing the pytorch and all other libraries related to the CNN model
import torch 
from torch.nn import Conv1d

from torch.nn import MaxPool1d

from torch.nn import Flatten
from torch.nn import Linear
from torch.nn import Sequential
from torch.nn.functional import relu
from torch.utils.data import DataLoader, TensorDataset

# defining the cnn regressor model and intialzing all the input layer , conv layer an flatten layer
class CnnRegressor(torch.nn.Module):
  def __init__(self, batch_size, inputs, outputs):
    super(CnnRegressor, self).__init__()
    self.batch_size = batch_size
    self.inputs = inputs
    self.outputs = outputs

    self.input_layer = Conv1d(inputs, batch_size, 1)

    self.max_pooling_layer = MaxPool1d(1)

    self.conv_layer = Conv1d(batch_size, 64, 1)
    #self.conv_layer1 = Conv1d(batch_size, 32, 1)

    self.flatten_layer = Flatten()

    self.non_linear_layer =  Sequential(Linear(64, 32))

    self.output_layer =  Sequential(Linear(32, outputs))

  def feed(self, input):
    input = input.reshape((self.batch_size, self.inputs, 1))

    output = relu(self.input_layer(input))

    output = relu(self.max_pooling_layer(output))

    output = relu(self.conv_layer(output))

    output = self.flatten_layer(output)

    output = self.non_linear_layer(output)

    output = self.output_layer(output)

    return output

# importing the pytorcgh ignite ans the gradient for the calculations
from torch.optim import SGD

from torch.nn import L1Loss

!pip install pytorch-ignite
from ignite.contrib.metrics.regression.r2_score import R2Score

# setting batch size
batch_size = 32
# calling the model 
model = CnnRegressor(batch_size, X.shape[1], 1)

model.cuda()

# defining a function for model loss and intializing the l1 loss and r2 score
def model_loss(model, dataset, train = False, optimizer = None):
  performance = L1Loss()
  score_metric = R2Score()

  avg_loss = 0
  avg_score = 0
  count = 0

  for input, output in iter(dataset):
    predictions = model.feed(input)
    loss = performance(predictions, output)
    score_metric.update([predictions, output])
    score = score_metric.compute()

    if(train):
      optimizer.zero_grad()

      loss.backward()

      optimizer.step()

    avg_loss += loss.item()
    avg_score += score
    count += 1
  
  return avg_loss / count, avg_score / count

# intializing epochs which will run in iteratioin for the whole data
epochs = 50
# inputting the learning rate for the gradient calculation
optimizer = SGD(model.parameters(), lr=1e-5)

inputs = torch.from_numpy(x_train_np).cuda().float()
outputs = torch.from_numpy(y_train_np.reshape(y_train_np.shape[0],1)).cuda().float()

tensor = TensorDataset(inputs, outputs)
loader = DataLoader(tensor, batch_size, shuffle=True, drop_last=True)

for epoch in range(epochs):
  avg_loss, avg_r2_score = model_loss(model, loader, train=True, optimizer=optimizer)

  print("Epoch " + str(epoch + 1) + ":\n\tLoss = " + str(avg_loss) + "\n\tR^2 Score = " + str(avg_r2_score))

  torch.save(model.state_dict(),'/content/1105500_1dconv reg' )

# displaying the final result after avg loss and avg r2 score
inputs = torch.from_numpy(x_test_np).cuda().float()
outputs = torch.from_numpy(y_test_np.reshape(y_test_np.shape[0],1)).cuda().float()

tensor = TensorDataset(inputs, outputs)
loader = DataLoader(tensor, batch_size, shuffle=True, drop_last=True)

avg_loss, avg_r2_score = model_loss(model, loader)
print("The model's L1 score is : " + str(avg_loss))
print("The model's R^2 score is : " + str(avg_r2_score))